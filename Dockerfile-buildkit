# syntax=docker/dockerfile:1.0.2-experimental

ARG BASE_IMAGE=nvcr.io/nvidia/tensorrtserver:19.06-py3
FROM ${BASE_IMAGE}

## INSTALL JFROG CLI
RUN mkdir -p /usr/local/bin && \
     cd /usr/local/bin && \
     curl -fL https://getcli.jfrog.io | sh

## CREATE MODEL REPOSITORY (https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/model_repository.html)
RUN mkdir -p /models

## DOWNLOAD MODELS
RUN mkdir -p /models/body_reid_v2.5.0/2/
RUN --mount=type=secret,id=jfrog-cfg,target=/root/.jfrog/jfrog-cli.conf \
  jfrog rt dl --flat nets/BodyFE/v2/V2.5.0_body_reid_2.onnx /models/body_reid_v2.5.0/2/model.onnx

RUN mkdir -p /models/body_detector_v4.2.3/1/
RUN --mount=type=secret,id=jfrog-cfg,target=/root/.jfrog/jfrog-cli.conf \
  jfrog rt dl --flat nets/BodyDetection/v4.2/pytorch/V4.2.3_body_det.onnx /models/body_detector_v4.2.3/1/model.onnx

## INSTALL ONNX-TENSORRT
RUN apt-get update \
 && apt-get install --no-install-recommends -y cmake libprotobuf-dev protobuf-compiler python
RUN git clone --recursive https://github.com/onnx/onnx-tensorrt.git /root/onnx-tensorrt \
 && mkdir -p /root/onnx-tensorrt/build \
 && cd /root/onnx-tensorrt/build \
 && cmake .. -DTENSORRT_ROOT="/usr/src/tensorrt" -DGPU_ARCHS="60 61 70 75" \
 && make VERBOSE=1 -j"$(nproc)" \
 && make install
